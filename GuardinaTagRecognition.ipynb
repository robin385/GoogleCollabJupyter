{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtext\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Tokenizer\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpreprocessing\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msequence\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pad_sequences\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, Flatten, Dense,StringLookup,GlobalAveragePooling1D\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "physical_devices = tf.config.list_physical_devices('GPU') \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True) \n",
    "\n",
    "file_path = os.path.join(os.getcwd(),'data_sets', 'all_Article_df.parquet')\n",
    "print(file_path)\n",
    "df = pd.read_parquet(file_path)\n",
    "print(df.columns)\n",
    "# Split the DataFrame based on the label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "df_dict = dict(tuple(df.groupby('label')))\n",
    "maxArticle=0\n",
    "label_word_frequency = {}\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "def preprocess_text(text):\n",
    "  # Lowercase the text\n",
    "  text = tf.strings.lower(text)\n",
    "  # Remove punctuation\n",
    "  text = tf.strings.regex_replace(text, r\"[^a-zA-Z0-9\\s]\", \"\")\n",
    "  # You can add more preprocessing steps like stemming or lemmatization (optional)\n",
    "  #text= tf.strings.split(text)\n",
    "  #text = [lemmatizer.lemmatize(word.numpy().decode('utf-8')) for word in text]\n",
    "  #text = tf.strings.join(text)\n",
    "\n",
    "\n",
    "  return text.numpy().decode('utf-8')\n",
    "\n",
    "max_article_length=0\n",
    "tokenizer = Tokenizer()\n",
    "count=0\n",
    "breakflag=False\n",
    "for label, articles_df in df_dict.items():\n",
    "    word_counts = tf.zeros(len(tokenizer.word_index), dtype=tf.int32)\n",
    "    for article in articles_df['Article']:\n",
    "        max_article_length=max(max_article_length,len(article))\n",
    "        count+=1\n",
    "        print(count, end='\\r', flush=True)\n",
    "        processed_article = preprocess_text(article)\n",
    "        if count>=854:\n",
    "            break\n",
    "        article_tokens = tokenizer.texts_to_sequences([processed_article])[0]\n",
    "        # Update word counts using the tokens\n",
    "        word_counts += tf.math.bincount(article_tokens)\n",
    "    # Split the article into words\n",
    "    count=0\n",
    "    label_word_frequency[label] = word_counts\n",
    "    if breakflag:\n",
    "        break\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(df['Article'])\n",
    "sequences = tokenizer.texts_to_sequences(df['Article'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "vocabulary_size = len(set(word for word_counts in label_word_frequency.values() for word in word_counts))\n",
    "embedding_dim = 128  # You can adjust this hyperparameter\n",
    "num_labels = len(df_dict)\n",
    "\n",
    "# Create the model\n",
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=vocabulary_size, output_dim=embedding_dim))  # Adjust max_article_length\n",
    "model.add(Flatten())\n",
    "model.add(Dense(num_labels, activation='softmax'))\n",
    "\n",
    "# Compile the model\n",
    "model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "# Step 7: Compile the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_and_batch(sequences):\n",
    "  padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(sequences, maxlen=max_article_length, padding='post')\n",
    "  return padded_sequences\n",
    "labels = df['label'].to_numpy()\n",
    "\n",
    "batch_size = 32  # Adjust batch size\n",
    "dataset = tf.data.Dataset.from_tensor_slices((sequences, labels))\n",
    "dataset = dataset.batch(batch_size)\n",
    "for epoch in range(10):\n",
    "  for batch_sequences, batch_labels in dataset:\n",
    "    padded_sequences = pad_and_batch(batch_sequences)\n",
    "    model.fit(padded_sequences, batch_labels, epochs=1) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
